'''requests служит для HTTP-запросов. BeautifulSoup - модуль, с помощью которого будет производиться парсинг. Модуль JSON - формирует json объект из словаря, мне так удобно хранить данные.
В первую очередь, мы собрали ссылки на продукты и сохранили в json файл.
Не забываем подключить библиотеки:
'''



import requests
from bs4 import BeautifulSoup
import json

#Получаем html-код и передаём его в переменную:

r = requests.get(url).text

#Записываем в переменную объект BeautifulSoup, то есть дерево синтаксического разбора этой страницы, полученной с помощью встроенного lxml:

soup = BeautifulSoup(html, 'lxml')

#Присваиваем переменной название файла:

filename = 'hyundai_links_products.json'

#Открываем файл:

wfile = open(filename, mode='w', encoding='UTF-8')

#С помощью soup получаем "лист" с ссылками товаров:

prod_block = soup.find('div', class_='product-list')

#Создаём словарь:

links = []

#Проходим циклом полученные данные, получаем href каждой ссылки и добавляем в словарь:

for block in prod_block.find_all('div', class_='tov'):
    links.append(block.find('div', class_='image').find('a').get('href'))

#Записываем полученные данные в json формате в файл и закрываем его:

json.dump(links, wfile, indent=4, ensure_ascii=False)
wfile.close()

'''Таким способом мы получаем html-код и записываем его в файл. Дальнейший парсинг производим из файла, а не напрямую со страницы. 
Это сэкономит наши ресурсы и меньше вероятность, что сервер нас заблокирует. После мы каждый файл обходим и получаем необходимые данные,
записываем в массив, массив преобразуем в json, для удобства и записываем в файл.'''
